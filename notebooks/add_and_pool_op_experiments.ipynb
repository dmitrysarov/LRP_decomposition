{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to popagate relevance in skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_func(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4888], requires_grad=True) tensor([10.]) tensor([4.8879], grad_fn=<MulBackward0>)\n",
      "tensor([0.2713], requires_grad=True) tensor([10.]) tensor([2.7127], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "c = add_func(a, b)\n",
    "c.backward(torch.tensor([10.]))\n",
    "print(a, a.grad, a*a.grad)\n",
    "print(b, b.grad, b*b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usual gradient propagation is not valid. Sum of relevances not equal to initial relevance 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Madd_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, func, a, b):\n",
    "        ctx.a = a.clone()\n",
    "        ctx.b = b.clone()\n",
    "        ctx.func = func\n",
    "        return func(a,b)\n",
    "    @staticmethod\n",
    "    def backward(ctx, R):\n",
    "        ctx.a.requires_grad_(True)\n",
    "        ctx.b.requires_grad_(True)\n",
    "        with torch.enable_grad():\n",
    "            Z = ctx.func(ctx.a, ctx.b)\n",
    "            S = R/Z\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "            Z.backward(S)\n",
    "        return None, ctx.a*ctx.a.grad,  ctx.b*ctx.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4330], requires_grad=True) tensor([4.1622])\n",
      "tensor([0.6073], requires_grad=True) tensor([5.8378])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "c = Madd_func.apply(torch.add, a, b)\n",
    "c.backward(torch.tensor([10.]))\n",
    "print(a, a.grad)\n",
    "print(b, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward step should be overloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to propagete relevance in pool layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7801, 0.9829, 0.3185, 0.1373, 0.7157],\n",
      "         [0.3277, 0.4798, 0.9149, 0.8780, 0.8434],\n",
      "         [0.4910, 0.6230, 0.7462, 0.8135, 0.6183],\n",
      "         [0.3260, 0.0618, 0.4656, 0.9856, 0.1830],\n",
      "         [0.7073, 0.5595, 0.9779, 0.4747, 0.1841]]], requires_grad=True)\n",
      "tensor([[[0.4000, 0.4000, 0.4000, 0.4000, 0.4000],\n",
      "         [0.4000, 0.4000, 0.4000, 0.4000, 0.4000],\n",
      "         [0.4000, 0.4000, 0.4000, 0.4000, 0.4000],\n",
      "         [0.4000, 0.4000, 0.4000, 0.4000, 0.4000],\n",
      "         [0.4000, 0.4000, 0.4000, 0.4000, 0.4000]]])\n",
      "tensor([[[0.3120, 0.3932, 0.1274, 0.0549, 0.2863],\n",
      "         [0.1311, 0.1919, 0.3659, 0.3512, 0.3374],\n",
      "         [0.1964, 0.2492, 0.2985, 0.3254, 0.2473],\n",
      "         [0.1304, 0.0247, 0.1862, 0.3942, 0.0732],\n",
      "         [0.2829, 0.2238, 0.3912, 0.1899, 0.0736]]], grad_fn=<MulBackward0>)\n",
      "tensor(5.8383, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,5,5, requires_grad=True)\n",
    "b = torch.nn.AdaptiveAvgPool2d((1,1))(a)\n",
    "print(a)\n",
    "b.backward(torch.tensor([10.]).view(b.shape))\n",
    "print(a.grad)\n",
    "print(a*a.grad)\n",
    "print((a*a.grad).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mavgpool_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, func, a, args_dict):\n",
    "        ctx.a = a.clone()\n",
    "        ctx.func = func\n",
    "        ctx.args_dict = args_dict\n",
    "        return func(a, **args_dict)\n",
    "    @staticmethod\n",
    "    def backward(ctx, R):\n",
    "        ctx.a.requires_grad_(True)\n",
    "        with torch.enable_grad():\n",
    "            Z = ctx.func(ctx.a, **ctx.args_dict)\n",
    "            S = R/Z\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "            Z.backward(S)\n",
    "        return None, ctx.a*ctx.a.grad, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and again backward step should be overloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1457, 0.8833, 0.4814, 0.0671, 0.6818],\n",
      "         [0.4463, 0.2085, 0.9480, 0.4361, 0.5725],\n",
      "         [0.7493, 0.1586, 0.4012, 0.1397, 0.8477],\n",
      "         [0.8180, 0.4504, 0.8729, 0.2440, 0.4761],\n",
      "         [0.4300, 0.6401, 0.0823, 0.2966, 0.6457]]], requires_grad=True)\n",
      "tensor([[[0.1202, 0.7286, 0.3971, 0.0553, 0.5624],\n",
      "         [0.3681, 0.1720, 0.7819, 0.3597, 0.4722],\n",
      "         [0.6181, 0.1308, 0.3309, 0.1152, 0.6992],\n",
      "         [0.6747, 0.3715, 0.7200, 0.2013, 0.3927],\n",
      "         [0.3547, 0.5280, 0.0679, 0.2447, 0.5326]]])\n",
      "tensor(10.0000)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,5,5, requires_grad=True)\n",
    "b = Mavgpool_func.apply(torch.nn.functional.adaptive_avg_pool2d, a, {'output_size': (1,1)})\n",
    "print(a)\n",
    "b.backward(torch.tensor([10.]).view(b.shape))\n",
    "print(a.grad)\n",
    "print(a.grad.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sum of obtained relevances is equal to initial relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
