{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import imp\n",
    "import torch\n",
    "import numpy as np\n",
    "utils = imp.load_source('util', '../utils.py')\n",
    "flatten_model = utils.flatten_model\n",
    "\n",
    "model = resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRP_ZRule(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, func, input, *args):\n",
    "        '''\n",
    "        forawd pass perform usual func forward pass\n",
    "        '''\n",
    "        ctx.func = func\n",
    "        ctx.input =input.clone().detach()\n",
    "        ctx.args = [*args]\n",
    "        return func(input, *args)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, R=1):\n",
    "        '''\n",
    "        substitute backward pass with z rule propagation \n",
    "        because forward pass given with 3 arguments, backward return 3 outputs\n",
    "        '''\n",
    "        ctx.input.requires_grad_(True)\n",
    "        with torch.enable_grad():\n",
    "            Z = ctx.func(ctx.input, *ctx.args)\n",
    "            S = R /(Z + (Z==0).float()*np.finfo(np.float32).eps)\n",
    "            Z.backward(S)\n",
    "            C = ctx.input.grad\n",
    "            R = ctx.input * C\n",
    "        return None, R, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]], grad_fn=<ViewBackward>)\n",
      "weights:\n",
      "  tensor([[[[0.9557, 0.2066],\n",
      "          [0.8205, 0.0062]]]])\n",
      "output: \n",
      " tensor([[[[1.9890, 1.9890, 1.9890],\n",
      "          [1.9890, 1.9890, 1.9890],\n",
      "          [1.9890, 1.9890, 1.9890]]]], grad_fn=<LRP_ZRuleBackward>)\n",
      "x grad: \n",
      " tensor([[[[0.4805, 0.5844, 0.5844, 0.1039],\n",
      "          [0.8930, 1.0000, 1.0000, 0.1070],\n",
      "          [0.8930, 1.0000, 1.0000, 0.1070],\n",
      "          [0.4125, 0.4156, 0.4156, 0.0031]]]])\n",
      "weights grad: \n",
      " tensor([[[[4.5249, 4.5249],\n",
      "          [4.5249, 4.5249]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,4, requires_grad=True).view(1,1,4,4)\n",
    "print('input: \\n', x)\n",
    "x.retain_grad()\n",
    "w = torch.rand(1,1, 2,2)\n",
    "print('weights:\\n ', w)\n",
    "w.requires_grad_(True)\n",
    "f = LRP_ZRule.apply(torch.conv2d, x, w)\n",
    "print('output: \\n', f)\n",
    "\n",
    "f.backward(torch.ones(1,1,3,3))\n",
    "\n",
    "print('x grad: \\n', x.grad)\n",
    "\n",
    "print('weights grad: \\n', w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w grad: \n",
      " tensor([[[[9., 9.],\n",
      "          [9., 9.]]]])\n",
      "x grad: \n",
      " tensor([[[[0.8868, 1.4965, 1.4965, 0.6097],\n",
      "          [1.7728, 2.4240, 2.4240, 0.6512],\n",
      "          [1.7728, 2.4240, 2.4240, 0.6512],\n",
      "          [0.8860, 0.9275, 0.9275, 0.0414]]]])\n",
      "tensor([[[[0.8868, 1.4965, 1.4965, 0.6097],\n",
      "          [1.7728, 2.4240, 2.4240, 0.6512],\n",
      "          [1.7728, 2.4240, 2.4240, 0.6512],\n",
      "          [0.8860, 0.9275, 0.9275, 0.0414]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,4, requires_grad=True).view(1,1,4,4)\n",
    "x.retain_grad()\n",
    "x.register_hook(lambda x: print('x grad: \\n',x))\n",
    "w = torch.rand(1,1,2,2)\n",
    "w.requires_grad_(True)\n",
    "w.register_hook(lambda x: print('w grad: \\n',x))\n",
    "c = torch.conv2d(x, w)\n",
    "c.backward(torch.ones(1,1,3,3))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.nn import modules\n",
    "import torch.nn.functional as F\n",
    "_ConvNd = modules.conv._ConvNd\n",
    "class MyLayer(_ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, padding_mode)\n",
    "\n",
    "    def conv2d_forward(self, input, weight):\n",
    "        if self.padding_mode == 'circular':\n",
    "            expanded_padding = ((self.padding[1] + 1) // 2, self.padding[1] // 2,\n",
    "                                (self.padding[0] + 1) // 2, self.padding[0] // 2)\n",
    "            return F.conv2d(F.pad(input, expanded_padding, mode='circular'),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv2d_forward(input, self.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): MyLayer(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): MyLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): MyLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): MyLayer(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): MyLayer(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): MyLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): MyLayer(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): MyLayer(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): MyLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): MyLayer(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): MyLayer(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): MyLayer(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): MyLayer(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for module in flatten_model(model):\n",
    "    if module.__class__.__name__ == 'Conv2d':\n",
    "#         print(module)\n",
    "        module.__class__ = MyLayer\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.rand(1,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f7a005c11d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.], requires_grad=True)\n",
    "a.register_hook(lambda x: print(x))\n",
    "b = 2*a\n",
    "c = b + a**2\n",
    "c.register_hook(lambda x: torch.tensor([5.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7f7a005c1160>, 0), (<PowBackward0 at 0x7f7a005c13c8>, 0))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9797, 0.9797, 0.9797],\n",
       "          [0.9797, 0.9797, 0.9797],\n",
       "          [0.9797, 0.9797, 0.9797]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.ones(4,4, dtype=torch.float32).view(1,1,4,4)\n",
    "r = torch.nn.Conv2d(1,1,(2,2))(d)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((None, 0),\n",
       " (<AccumulateGrad at 0x7f7a005c1be0>, 0),\n",
       " (<AccumulateGrad at 0x7f7a005c1a58>, 0))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
