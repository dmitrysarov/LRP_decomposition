{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient in conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w grad: \n",
      " tensor([[[[ 9., 12.,  9.],\n",
      "          [12., 16., 12.],\n",
      "          [ 9., 12.,  9.]]]])\n",
      "x grad: \n",
      " tensor([[[[1.6835, 2.8811, 2.8811, 1.7759],\n",
      "          [2.8740, 4.8261, 4.8261, 2.7601],\n",
      "          [2.8740, 4.8261, 4.8261, 2.7601],\n",
      "          [2.3029, 3.6182, 3.6182, 1.6654]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,4, requires_grad=True).view(1,1,4,4)\n",
    "x.retain_grad()\n",
    "x.register_hook(lambda x: print('x grad: \\n',x))\n",
    "w = torch.rand(1,1,3,3)\n",
    "w.requires_grad_(True)\n",
    "w.register_hook(lambda x: print('w grad: \\n',x))\n",
    "c = torch.conv2d(x, weight=w, padding=1, stride=1, dilation=1)\n",
    "c.backward(torch.ones(1,1,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 5])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NotImplemented"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRP in backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRP_ZRule_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, func, input, *args):\n",
    "        '''\n",
    "        forawd pass perform usual func forward pass\n",
    "        '''\n",
    "        ctx.func = func\n",
    "        ctx.input =input.clone().detach()\n",
    "        ctx.args = [*args]\n",
    "        return func(input, *args)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, R):\n",
    "        '''\n",
    "        substitute backward pass with z rule propagation \n",
    "        backward pass must return same namber of ouputs as number of inputs\n",
    "        '''\n",
    "        ctx.input.requires_grad_(True)\n",
    "        with torch.enable_grad():\n",
    "            Z = ctx.func(ctx.input, *ctx.args)\n",
    "            S = R /(Z + (Z==0).float()*np.finfo(np.float32).eps)\n",
    "            Z.backward(S)\n",
    "            C = ctx.input.grad\n",
    "            R = ctx.input * C\n",
    "        return tuple([None, R] + len(ctx.args)*[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " tensor([[[[-1., -1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  1.],\n",
      "          [-1., -1.,  1.,  1.]]]], grad_fn=<ViewBackward>)\n",
      "weights:\n",
      "  tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "f: \n",
      " tensor([[[[-4., -2.,  2.,  4.],\n",
      "          [-6., -3.,  3.,  6.],\n",
      "          [-6., -3.,  3.,  6.],\n",
      "          [-4., -2.,  2.,  4.]]]], grad_fn=<LRP_ZRule_funcBackward>)\n",
      "out: \n",
      " tensor(0., grad_fn=<SumBackward0>)\n",
      "f backward inpt: \n",
      " tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]])\n",
      "x relevance: \n",
      " tensor([[[[1.2500, 0.4167, 0.4167, 1.2500],\n",
      "          [1.7500, 0.5833, 0.5833, 1.7500],\n",
      "          [1.7500, 0.5833, 0.5833, 1.7500],\n",
      "          [1.2500, 0.4167, 0.4167, 1.2500]]]])\n",
      "weights grad: \n",
      " tensor([[[[0.5833, 3.5000, 0.5833],\n",
      "          [0.8333, 5.0000, 0.8333],\n",
      "          [0.5833, 3.5000, 0.5833]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[-1.,-1.,1.,1.]]*4, requires_grad=True).view(1,1,4,4)\n",
    "print('input: \\n', x)\n",
    "x.retain_grad()\n",
    "w = torch.tensor([[1.,1.,1.]]*3).view(1,1,3,3)\n",
    "print('weights:\\n ', w)\n",
    "w.requires_grad_(True)\n",
    "f = LRP_ZRule_func.apply(torch.conv2d, x, w, None, 1, 1)\n",
    "f.register_hook(lambda x: print('f backward inpt: \\n', x))\n",
    "print('f: \\n', f)\n",
    "\n",
    "out = f.sum()\n",
    "\n",
    "print('out: \\n', out)\n",
    "out.backward()\n",
    "\n",
    "print('x relevance: \\n', x.grad)\n",
    "\n",
    "print('weights grad: \\n', w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip connection emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x input grad: \n",
      " tensor([[[[1.6944, 1.9722, 1.9722, 1.6944],\n",
      "          [1.9722, 2.3611, 2.3611, 1.9722],\n",
      "          [1.9722, 2.3611, 2.3611, 1.9722],\n",
      "          [1.6944, 1.9722, 1.9722, 1.6944]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,4,requires_grad=True).view(1,1,4,4)\n",
    "x.register_hook(lambda x: print('x input grad: \\n', x))\n",
    "w = torch.ones(1,1,3,3)\n",
    "\n",
    "f = LRP_ZRule.apply(torch.conv2d, x, w, None, 1, 1)\n",
    "\n",
    "c = f + x\n",
    "c.backward(torch.ones(1,1,4,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x input grad: \n",
      " tensor([[[[0.6944, 0.9722, 0.9722, 0.6944],\n",
      "          [0.9722, 1.3611, 1.3611, 0.9722],\n",
      "          [0.9722, 1.3611, 1.3611, 0.9722],\n",
      "          [0.6944, 0.9722, 0.9722, 0.6944]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,4,requires_grad=True).view(1,1,4,4)\n",
    "x.register_hook(lambda x: print('x input grad: \\n', x))\n",
    "w = torch.ones(1,1,3,3)\n",
    "\n",
    "f = LRP_ZRule.apply(torch.conv2d, x, w, None, 1, 1)\n",
    "\n",
    "c = f\n",
    "c.backward(torch.ones(1,1,4,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4085, -0.1275,  0.0120,  0.2322],\n",
       "          [-0.4939,  0.2286,  0.4515,  0.4954],\n",
       "          [-0.5135,  0.1242,  0.4497,  0.5450],\n",
       "          [-0.7433, -0.2653, -0.0550,  0.0685]]]])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.tensor([[[[0.7597, 1.0883, 0.9689, 0.3407],\n",
    "          [1.2799, 1.3725, 1.2638, 0.5784],\n",
    "          [1.2476, 1.4671, 1.3128, 0.5307],\n",
    "          [1.1303, 1.1453, 1.0129, 0.5011]]]]) + \n",
    "torch.tensor([[[[0.3512, 0.9608, 0.9809, 0.5729],\n",
    "          [0.7860, 1.6011, 1.7153, 1.0738],\n",
    "          [0.7341, 1.5913, 1.7625, 1.0757],\n",
    "          [0.3870, 0.8800, 0.9579, 0.5696]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5382, 0.2435, 0.8232],\n",
       "          [0.2217, 0.0189, 0.4675],\n",
       "          [0.5245, 0.3569, 0.6152]]]])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
